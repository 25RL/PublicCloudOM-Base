[TOC]







# 1. 云硬盘

## 1.1 云硬盘概念

https://www.huaweicloud.com/product/evs.html
1. 提供高可靠、高性能、规格丰富且可弹性扩展的块存储服务，挂载至ECS、BMS等计算服务使用，持久化存储数据。





## 1.2 云硬盘分类

云硬盘属性分为：系统盘和数据盘

1. 系统盘：是虚拟机的 “启动盘”，容量小但对随机读写性能敏感，需优先保证稳定性和响应速度。
2. 数据盘：是 “数据仓库”，容量按需分配，性能需求随业务场景变化，需结合成本和业务类型选择存储类型。

| 维度               | 系统盘                                                       | 数据盘                                                       |
| ------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 核心功能           | 安装操作系统运行必要的程序、配置文件                         | 存储用户业务数据(如文档、数据库文件、日志、应用程序等)       |
| 与虚拟机的绑定关系 | 与虚拟机 “生命周期绑定”：创建虚拟机时必须指定系统盘，删除虚拟机时，系统盘通常会随虚拟机一同删除（除非设置 “保留系统盘”）。 | 与虚拟机 “松绑定”：可独立创建、挂载、卸载或迁移，即使虚拟机删除，数据盘仍可保留（需手动操作） |
| 默认配置           | 公有云通常会为虚拟机分配默认大小的系统盘（如 20-50GB），满足基础系统运行需求。 | 需用户根据需求手动添加，默认不分配（部分场景可能赠送小容量数据盘，但需手动启用）。 |







# 2. 系统盘和数据盘-容量和性能要求的差异

## 2.1 系统盘

- **容量要求**：
  主要取决于操作系统类型和系统程序的大小，通常无需太大。例如：
  - Linux 系统（如 CentOS、Ubuntu）：20-50GB 即可满足基础需求；
  - Windows Server 系统：因系统文件较大，建议 50-100GB。
    若需安装大量系统级软件（如数据库服务、大型中间件），可适当扩容，但一般不建议超过 200GB（否则应考虑将软件数据迁移到数据盘）。
- **性能要求**：
  系统盘性能直接影响虚拟机的启动速度、系统响应速度（如命令执行、程序加载），因此对**随机读写性能**（尤其是随机读）要求较高。
  公有云通常默认提供较高性能的存储类型（如 SSD）作为系统盘，避免因系统盘性能不足导致虚拟机卡顿。

## 2.2 数据盘

- **容量要求**：
  完全取决于用户业务的数据量，差异极大。例如：
  - 个人博客或小型网站：可能仅需 100-500GB；
  - 企业级数据库、视频存储、日志服务器：可能需要数 TB 甚至数十 TB。
    数据盘容量需根据业务增长预期提前规划，公有云支持动态扩容（多数无需停机），方便后续调整。
- **性能要求**：
  性能需求与数据的读写模式密切相关：
  - 若用于数据库（如 MySQL、PostgreSQL）、高频交易系统：需**高 IOPS（每秒输入输出操作数）和低延迟**，适合选择 SSD 或本地 NVMe 存储；
  - 若用于冷数据归档（如历史日志、备份文件）：对性能要求低，可选择低成本的对象存储或低频访问硬盘（如 SATA 盘）；
  - 若用于视频流、大文件传输：更关注**吞吐量（带宽）**，需确保连续读写速度满足需求。



**使用建议**：

1. 系统盘仅保留操作系统和必要的系统程序，避免存储业务数据（防止虚拟机删除时数据丢失）；
2. 数据盘根据数据类型选择存储类型（SSD/ HDD/ 对象存储），平衡性能与成本；
3. 定期备份系统盘（避免系统崩溃）和数据盘（防止数据丢失），公有云通常提供快照功能支持。







# 3. 性能

## 3.1 核心指标

存储性能的核心指标包括**IOPS、吞吐量、延迟**，此外还有 IO 模式（随机 / 顺序）、队列长度等辅助指标，具体定义及意义如下：

| **指标**                       | **定义**                                                     | **对系统盘的影响**                                           | **对数据盘的影响**                                           |
| ------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **IOPS**（每秒输入输出操作数） | 每秒完成的读写操作次数，分**随机 IOPS**（小文件 / 分散地址读写，如数据库）和**顺序 IOPS**（大文件 / 连续地址读写，如视频）。 | 系统盘以随机 IO 为主（如加载系统文件、执行命令），随机 IOPS 不足会导致系统卡顿、启动慢。 | 取决于业务：数据库需高随机 IOPS；视频存储需高顺序 IOPS。     |
| **吞吐量**（带宽）             | 每秒读写的数据总量（单位 MB/s 或 GB/s），反映连续读写能力。  | 系统盘对吞吐量要求较低（系统文件多为小文件）。               | 大文件传输（如视频、备份）依赖高吞吐量，不足会导致传输速度慢。 |
| **延迟**（响应时间）           | 从发出读写请求到完成的时间（单位 ms），分读延迟和写延迟。    | 延迟直接影响系统响应速度（如命令执行、程序启动），高延迟会导致操作卡顿。 | 低延迟对实时业务（如高频交易、在线数据库）至关重要，高延迟会拖慢业务处理速度。 |
| **IO 队列长度**                | 等待处理的 IO 请求数量，过长会导致延迟升高（请求排队）。     | 系统盘队列长度通常较低，若过高可能因系统进程频繁读写导致。   | 数据盘队列长度需与业务匹配（如数据库可容忍稍长队列，但需避免过载）。 |
| **IOPS 抖动**                  | 短时间内 IOPS 的波动幅度，稳定性差会导致业务性能不稳定。     | 系统盘抖动过大会导致虚拟机忽快忽慢。                         | 对稳定性要求高的业务（如游戏服务器）影响显著。               |



## 3.2 性能评估方法

1. 通用测试工具（适用于系统盘和数据盘）

- **fio**（Linux/Unix）：最常用的存储性能测试工具，可模拟不同 IO 模式（随机 / 顺序、读 / 写）。
  - 示例 1（测试系统盘随机读性能）：

```bash
fio -filename=/testfile -direct=1 -iodepth=16 -thread -rw=randread -ioengine=libaio -bs=4k -size=10G -numjobs=1 -runtime=60 -group_reporting -name=randread_test
```

示例 2（测试数据盘顺序写吞吐量）：

```bash
fio -filename=/data/testfile -direct=1 -iodepth=8 -thread -rw=write -ioengine=libaio -bs=1024k -size=100G -numjobs=1 -runtime=120 -group_reporting -name=seqwrite_test
```

（参数说明：`write`为顺序写，`bs=1024k`模拟大文件，测试吞吐量）

- **iostat**（Linux）：实时监控 IO 负载，查看 IOPS、吞吐量、延迟等。
  命令：`iostat -x 5`（每 5 秒输出一次详细统计，关注`r/s`（读 IOPS）、`w/s`（写 IOPS）、`rkB/s`/`wkB/s`（吞吐量）、`await`（平均延迟））。
- **性能监视器**（Windows）：通过 “磁盘” 计数器监控，如 “每秒磁盘 IO 操作数”（IOPS）、“平均磁盘秒 / 读”（读延迟）、“磁盘吞吐量” 等。



2. 云厂商自带监控工具

公有云平台（如 AWS CloudWatch、阿里云云监控、腾讯云监控）提供存储性能 metrics 监控，可直接查看：

- 系统盘 / 数据盘的 IOPS、吞吐量、延迟实时曲线；
- 性能瓶颈告警（如 IOPS 达到上限、延迟过高）。
  **优势**：无需在虚拟机内部署工具，可长期跟踪业务实际运行时的性能表现（比工具测试更贴近真实场景）



3. 评估注意事项

- 系统盘评估：重点测试**随机读写（4k 块大小）**，模拟系统启动、程序加载等场景；
- 数据盘评估：根据业务类型选择测试模式（如数据库用 4k 随机读写，视频存储用 1M 顺序读写）；
- 避免干扰：测试时关闭其他无关进程，确保结果准确；
- 结合业务指标：若实际业务（如数据库查询）卡顿，但工具测试性能正常，可能是应用层问题（如 SQL 语句未优化），需联动排查。





## 3.3 性能优化策略

1. 系统盘优化

系统盘性能直接影响虚拟机基础运行，优化核心是**保证随机读写效率和稳定性**：

- **选择合适的存储类型**：优先使用云厂商提供的 “高性能存储”（如阿里云 ESSD、AWS gp3）作为系统盘，避免用低性能的 HDD（可能导致启动慢、系统卡顿）。
- **控制系统盘占用**：
  - 不将业务数据、日志文件存放在系统盘（默认路径如`/var/log`可迁移到数据盘）；
  - 定期清理系统垃圾（如 Linux 的`yum clean`、Windows 的磁盘清理），避免磁盘满导致 IO 性能下降。
- **优化文件系统**：
  - Linux 系统建议用`ext4`或`xfs`（支持动态扩容，性能更稳定），避免老旧的`ext3`；
  - 挂载时添加参数（如`noatime`，禁止记录文件访问时间，减少不必要的写操作）。
- **减少系统进程 IO 消耗**：关闭无用的系统服务（如 Windows 的冗余服务、Linux 的不必要守护进程），避免后台进程频繁读写系统盘。



2. 数据盘优化

数据盘优化需**结合业务场景**，平衡性能、成本和稳定性：

| **业务场景**             | 性能瓶颈点           | 优化策略                                                     |
| ------------------------ | -------------------- | ------------------------------------------------------------ |
| 数据库（MySQL、Oracle）  | 高 IOPS 需求、低延迟 | -  选择 SSD 或 NVMe 存储（如 AWS io2、腾讯云 SSD 云盘）； <br />- 启用存储缓存（如数据库的 Buffer Pool）；<br /> - 分库分表减少单盘 IO 压力； - 避免频繁小文件读写（合并批量操作）。 |
| 视频 / 大文件存储        | 吞吐量不足           | - 选择高吞吐量存储（如阿里云吞吐量型 SSD）；<br /> - 启用文件分片（如 HDFS），并行读写； <br />- 避免跨区域读写（减少网络延迟对吞吐量的影响）。 |
| 冷数据归档（日志、备份） | 成本敏感，性能要求低 | - 迁移到低频访问存储（如 AWS S3 Infrequent Access、阿里云归档存储）； <br />- 压缩数据（如用 gzip、lz4 减少存储量和 IO 次数）。 |
| 高频交易 / 实时计算      | 延迟过高             | - 选择本地 NVMe 存储（如 AWS EC2 本地实例存储）； <br />- 减少 IO 队列长度（通过应用层控制并发请求数）； <br />- 启用存储多路径（提高冗余和吞吐量）。 |





3. 通用优化技巧

- **动态调整存储规格**：公有云支持在线升级数据盘类型（如从 HDD 换为 SSD）或扩容，业务高峰前提前操作。
- **合理使用缓存**：利用操作系统缓存（Page Cache）或应用层缓存（如 Redis），减少对磁盘的直接读写。
- **定期清理碎片**：Windows 系统可执行磁盘碎片整理；Linux 的 ext4/xfs 文件系统碎片较少，但长期大量小文件读写后可通过`xfs_fsr`优化。
- **结合快照与备份**：性能优化时先创建快照，避免操作失误导致数据丢失；同时将冷数据迁移到低成本存储，释放高性能盘资源。







# 4. 指标

```markdown
https://www.ctyun.cn/document/10027696/10029239

平均I/O服务时长	该指标用于统计测量对象在测量周期内平均每个读I/O或写I/O的服务时长。
单位：ms/op

磁盘I/O使用率	该指标用于统计测量对象云硬盘I/O使用率。
单位：%

平均队列长度	该指标用于统计测量对象在测量周期内平均等待完成的读取或写入操作请求的数量。
单位：count/op
```

## 4.1 平均 I/O 服务时长（Average I/O Service Time）

- **定义**：指完成一次 I/O 请求（包括读或写）所消耗的总时间，单位通常为毫秒（ms），包含磁盘寻道、旋转延迟和数据传输的时间总和。

- 反映的问题：

  - 直接体现磁盘处理单个 I/O 请求的效率。数值越低，说明磁盘响应速度越快。

  - 若数值过高（例如机械硬盘超过 20ms，SSD 超过 1ms），可能意味着：

    - 机械硬盘存在频繁寻道（随机 I/O 过多）或磁盘老化；
    - SSD 出现磨损、缓存不足或内部垃圾回收机制触发；
    - 磁盘硬件故障（如坏道）。

    

## 4.2 磁盘 I/O 使用率（Disk I/O Utilization）

- **定义**：指磁盘在单位时间内处于活跃状态（处理 I/O 请求）的时间占比，通常以百分比表示。
- 反映的问题：
  - 体现磁盘的繁忙程度。例如使用率 80% 意味着磁盘大部分时间在处理请求。
  - 若使用率长期接近 100%，说明磁盘已处于满负荷状态，新请求需要排队等待，会导致响应延迟增加（此时即使平均服务时长正常，整体性能也会下降）。
  - 需注意：使用率高不一定绝对是问题，需结合业务对延迟的敏感度（例如后台备份场景可容忍较高使用率，而数据库场景则需严格控制）。



## 4.3 平均队列长度（Average Queue Length）

- **定义**：指单位时间内等待处理的 I/O 请求数量，反映请求的积压程度。
- 反映的问题：
  - 体现磁盘的负载压力和处理能力的匹配度。
  - 若队列长度持续过高（例如机械硬盘超过 2-3，SSD 超过 10），说明磁盘处理速度跟不上请求产生的速度，请求需要排队等待，会直接导致平均服务时长增加，进而影响业务响应速度。
  - 队列长度与磁盘类型相关：机械硬盘因物理结构限制，队列长度过高会导致寻道时间大幅增加；而 SSD 并行处理能力强，可容忍更高的队列长度。





## 4.4 三者的关联与综合判断

- 若**平均队列长度高**且**使用率接近 100%**，说明磁盘已成为瓶颈，请求积压严重，需通过升级磁盘类型（如机械硬盘换 SSD）或增加磁盘数量（如 RAID）解决。
- 若**平均服务时长高**但队列长度和使用率正常，可能是磁盘硬件性能下降（如碎片化、坏道）或单个请求过大（如大文件读写），需优化 I/O 模式（如拆分请求）或检查磁盘健康状态。
- 若**使用率中等但队列长度高**，可能是请求突发集中（如业务高峰期），需通过缓存（如增加内存缓存）或流量控制减少请求压力。

通过三者的组合分析，可精准定位磁盘 I/O 性能问题的根源（是磁盘能力不足、请求模式不合理还是突发负载导致），为优化提供方向。











# 5. 案例

```
IOPS/吞吐量(MB/S)
```

1. **IOPS(2600)**

这块盘每秒最多能完成 2600 次 4 KiB 左右的随机读写操作.

- 类比：像 2600 个快递小哥同时收发小包裹，适合数据库、ETCD、ZooKeeper 这类“小而频繁”的 IO 模型。
- 云盘选型：AWS gp3 默认 3000 IOPS，阿里云 ESSD PL0 起步 3000，2600 在入门层够用，但跑 MySQL 高并发可能吃紧。



2.**吞吐量（140 MB/s）**

顺序大文件读写时的带宽上限。

- 类比：像一条 140 MB/s 的传送带搬大箱子，适合日志归档、Kafka 数据刷盘、镜像仓库上传下载。
- 云盘选型：AWS gp3 默认 125 MB/s，阿里云 PL0 约 150 MB/s，这条值处于“够用但不高”区间。



3.**两者关系**

2600 IOPS × 4 KiB ≈ 10.4 MB/s，远低于 140 MB/s，说明这块盘是 **IOPS 优先型** 而非带宽型。
如果业务突然变成大文件顺序写（比如备份），IOPS 不会成为瓶颈，但带宽 140 MB/s 可能先顶满。



4.**运维落地建议**：

- 监控：用 `iostat -xz 1` 看 `%util` 和 `await`，IOPS 长期 > 2000 或 await > 10 ms 就考虑升配。
- 调优：对延迟敏感的服务（如 Redis），把 `noop` 调度器换成 `none` 或 `mq-deadline`，降低 CPU 上下文切换。
- 扩容：云上可直接改卷类型（ESSD PL0→PL1）或加 IOPS，无需停机；记得做压测验证 `fio -randread -iodepth=32`。



一句话总结：**这块盘适合小 IO 频繁的业务，跑数据库够用，跑大数据管道会卡在带宽，必要时升配或换盘型。**



